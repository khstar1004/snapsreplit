import requests
from typing import List, Dict, Any
import random
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain
from langchain.text_splitter import CharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings
from langchain.chains import RetrievalQA
from langchain.schema import Document
from typing import List, Dict, Any, Optional
import os
import time
from datetime import datetime, timedelta
from requests.exceptions import RequestException
import logging
from bson.objectid import ObjectId
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.document_loaders import PyPDFLoader, DirectoryLoader
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain
from langchain.prompts import PromptTemplate
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler


load_dotenv()
logger = logging.getLogger(__name__)

class InstagramAPI:
    BASE_URL = "https://graph.instagram.com/v12.0"

    def __init__(self, access_token=None):
        self.access_token = access_token

    def set_access_token(self, access_token):
        """ì•¡ì„¸ìŠ¤ í† í°ì„ ì„¤ì •í•˜ëŠ” ë©”ì„œë“œ"""
        self.access_token = access_token

    def get_user_media(self, limit: int = 10) -> List[Dict[str, Any]]:
        """ì‚¬ìš©ìžì˜ Instagram ë¯¸ë””ì–´ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
        if not self.access_token:
            raise ValueError("Access token is not set")
            
        endpoint = f"{self.BASE_URL}/me/media"
        params = {
            "fields": "id,caption,media_type,media_url,thumbnail_url,permalink,timestamp",
            "access_token": self.access_token,
            "limit": limit
        }
        try:
            response = requests.get(endpoint, params=params)
            response.raise_for_status()
            return response.json().get("data", [])
        except requests.RequestException as e:
            print(f"Error fetching user media: {str(e)}")
            return []

    def format_posts(self, media_items: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """API ì‘ë‹µì„ ë³´ê¸° ì¢‹ê²Œ í¬ë§·íŒ…í•©ë‹ˆë‹¤."""
        formatted_posts = []
        for item in media_items:
            formatted_post = {
                "id": item.get("id"),
                "media_url": item.get("media_url") or item.get("thumbnail_url"),
                "caption": item.get("caption", "No caption"),
                "media_type": item.get("media_type"),
                "permalink": item.get("permalink"),
                "timestamp": item.get("timestamp")
            }
            formatted_posts.append(formatted_post)
        return formatted_posts

    def authenticate(self, code: str) -> Dict[str, str]:
        """Instagram OAuth ì¸ì¦ ì½”ë“œë¡œ ì•¡ì„¸ìŠ¤ í† í°ì„ íšë“í•©ë‹ˆë‹¤."""
        token_url = "https://api.instagram.com/oauth/access_token"
        data = {
            "client_id": os.getenv("INSTAGRAM_CLIENT_ID"),
            "client_secret": os.getenv("INSTAGRAM_CLIENT_SECRET"),
            "grant_type": "authorization_code",
            "redirect_uri": os.getenv("INSTAGRAM_REDIRECT_URI"),
            "code": code
        }
        
        try:
            response = requests.post(token_url, data=data)
            response.raise_for_status()
            return response.json()
        except requests.RequestException as e:
            raise Exception(f"Failed to authenticate: {str(e)}")

class ThreadAPI:
    def __init__(self):
        self.base_url = "https://graph.threads.net/v1.0"
        self.access_token = None

    def get_user_media(self, user_id):
        """ì‚¬ìš©ìžì˜ ëª¨ë“  Thread ê²Œì‹œë¬¼ ì¡°íšŒ"""
        try:
            endpoint = f"{self.base_url}/{user_id}/threads"
            params = {
                'fields': 'id,media_product_type,media_type,media_url,permalink,username,text,timestamp,shortcode,thumbnail_url,children,is_quote_post',
                'access_token': self.access_token
            }
            response = requests.get(endpoint, params=params)
            response.raise_for_status()
            data = response.json()

            # ê° ê²Œì‹œë¬¼ì˜ ë¯¸ë””ì–´ ì •ë³´ ì²˜ë¦¬
            posts = data.get('data', [])
            for post in posts:
                if post.get('media_type') in ['IMAGE', 'VIDEO', 'CAROUSEL_ALBUM']:
                    # ë¹„ë””ì˜¤ì¸ ê²½ìš° ì¸ë„¤ì¼ URL ì¶”ê°€
                    if post.get('media_type') == 'VIDEO' and post.get('thumbnail_url'):
                        post['media_preview'] = post['thumbnail_url']
                    
                    # ìºëŸ¬ì…€ì¸ ê²½ìš° ëª¨ë“  ë¯¸ë””ì–´ URL ìˆ˜ì§‘
                    if post.get('media_type') == 'CAROUSEL_ALBUM' and post.get('children'):
                        post['carousel_media'] = [child.get('media_url') for child in post['children']['data']]

            return data
        except Exception as e:
            logger.error(f"Error fetching Thread media: {str(e)}")
            return {'data': []}

    def get_media_insights(self, media_id):
        """íŠ¹ì • Thread ê²Œì‹œë¬¼ì˜ ì¸ì‚¬ì´íŠ¸ ì¡°íšŒ"""
        endpoint = f"{self.base_url}/{media_id}/insights"
        params = {
            'metric': 'views,likes,replies,reposts,quotes,shares',
            'access_token': self.access_token
        }
        response = requests.get(endpoint, params=params)
        return response.json()

    def get_user_insights(self, user_id):
        """ì‚¬ìš©ìžì˜ Thread ê³„ì • ì¸ì‚¬ì´íŠ¸ ì¡°íšŒ"""
        try:
            endpoint = f"{self.base_url}/{user_id}/threads_insights"
            params = {
                'metric': 'views,likes,replies,reposts,quotes,followers_count',
                'period': 'day',  # ì¼ë³„ ë°ì´í„° ìš”ì²­
                'access_token': self.access_token
            }
            response = requests.get(endpoint, params=params)
            response_data = response.json()
            
            # ê¸°ë³¸ í†µê³„ ë°ì´í„° êµ¬ì¡° ì´ˆê¸°í™”
            stats = {
                'total_posts': 0,
                'total_likes': 0,
                'total_replies': 0,
                'avg_likes': 0,
                'avg_replies': 0,
                'dates': [],
                'likes_data': [],
                'replies_data': []
            }

            # ê²Œì‹œë¬¼ ìˆ˜ ì¡°íšŒ
            posts_response = self.get_user_media(user_id)
            stats['total_posts'] = len(posts_response.get('data', []))

            # ìµœê·¼ 30ì¼ ë°ì´í„° ìƒì„±
            current_date = datetime.now()
            for i in range(30):
                date = (current_date - timedelta(days=i)).strftime('%Y-%m-%d')
                stats['dates'].insert(0, date)
                stats['likes_data'].insert(0, random.randint(10, 100))  # ìž„ì‹œ ë°ì´í„°
                stats['replies_data'].insert(0, random.randint(5, 50))  # ìž„ì‹œ ë°ì´í„°

            # ì´ê³„ ë° í‰ê·  ê³„ì‚°
            stats['total_likes'] = sum(stats['likes_data'])
            stats['total_replies'] = sum(stats['replies_data'])
            stats['avg_likes'] = round(stats['total_likes'] / len(stats['likes_data']))
            stats['avg_replies'] = round(stats['total_replies'] / len(stats['replies_data']))

            return stats
        except Exception as e:
            logger.error(f"Error fetching Thread insights: {str(e)}")
            return {
                'total_posts': 0,
                'total_likes': 0,
                'total_replies': 0,
                'avg_likes': 0,
                'avg_replies': 0,
                'dates': [],
                'likes_data': [],
                'replies_data': []
            }

    def post_thread(self, user_id, content, media_type='TEXT', media_url=None):
        """Thread ê²Œì‹œë¬¼ ìž‘ì„±"""
        endpoint = f"{self.base_url}/{user_id}/threads"
        data = {
            'text': content,
            'media_type': media_type,
            'access_token': self.access_token
        }
        
        if media_url:
            data['media_url'] = media_url
            
        response = requests.post(endpoint, json=data)
        return response.json()

def convert_post(caption: str, target_platform: str, has_image: bool) -> str:
    converted_post = caption

    if target_platform == "Twitter":
        converted_post = f"Check out my latest Instagram post! ðŸ“¸\n\n{caption[:200]}..." if len(caption) > 200 else caption
        converted_post += "\n\n#Instagram #Social"
    elif target_platform == "LinkedIn":
        converted_post = f"I just shared a new post on Instagram! Here's a sneak peek:\n\n{caption}\n\nFollow me on Instagram for more updates!"
        converted_post += "\n\n#SocialMedia #Professional #Instagram"
    elif target_platform == "Facebook":
        converted_post = f"New Instagram Post Alert! ðŸš¨\n\n{caption}\n\nHead over to my Instagram profile to see the full post and more content!"
    elif target_platform == "Thread":
        converted_post = f"Continuing from my recent Instagram post...\n\n{caption[:100]}...\n\nThoughts?"
    elif target_platform == "YouTube Community":
        converted_post = f"ðŸ“¸ Instagram Update ðŸ“¸\n\n{caption[:150]}...\n\nCheck out my Instagram for the full post and more behind-the-scenes content!"

    if has_image:
        converted_post += "\n\n[Image from Instagram]"

    return converted_post

class RAGConverter:
    def __init__(self):
        openai_api_key = os.getenv("OPENAI_API_KEY")
        if not openai_api_key:
            raise ValueError("OPENAI_API_KEY not found in .env file")
        
        self.llm = ChatOpenAI(
            temperature=0.7,
            model_name="gpt-4o-mini",
            openai_api_key=openai_api_key,
            streaming=True,
            callbacks=[StreamingStdOutCallbackHandler()]
        )
        
        # ìž„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” (ada-002 ëª…ì‹œì  ì§€ì •)
        self.embeddings = OpenAIEmbeddings(
            openai_api_key=openai_api_key,
            model="text-embedding-ada-002"
        )
        
        # ê°œì„ ëœ í…ìŠ¤íŠ¸ ë¶„í•  ì „ëžµ
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=500,  # ì²­í¬ í¬ê¸° ì¡°ì •
            chunk_overlap=50,
            length_function=len,
            separators=["\n\n", "\n", ".", "!", "?", " ", ""]
        )
        
        # ë©”ëª¨ë¦¬ ì´ˆê¸°í™” (ëŒ€í™” ì´ë ¥ ìœ ì§€)
        self.memory = ConversationBufferMemory(
            memory_key="chat_history",
            return_messages=True,
            output_key="answer"
        )
        
        # ë²¡í„° DB ì´ˆê¸°í™”
        self.vector_store = self._initialize_vector_store()
        
        if self.vector_store:
            # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜
            self.qa_prompt = PromptTemplate(
                template="""ë‹¹ì‹ ì€ ì†Œì…œ ë¯¸ë””ì–´ ë§ˆì¼€íŒ… ì „ë¬¸ê°€ìž…ë‹ˆë‹¤.
                
                ì£¼ì–´ì§„ ì»¨í…ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹¤ìŒ ì½˜í…ì¸ ë¥¼ ë³€í™˜í•´ì£¼ì„¸ìš”:
                
                ì»¨í…ìŠ¤íŠ¸:
                {context}
                
                í”Œëž«í¼: {platform}
                ì›ë³¸ ì½˜í…ì¸ : {question}
                ì´ë¯¸ì§€ í¬í•¨ ì—¬ë¶€: {has_image}
                
                ê° í”Œëž«í¼ì˜ íŠ¹ì„±ì„ ê³ ë ¤í•˜ì—¬ ìµœì í™”ëœ ì½˜í…ì¸ ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.
                ì´ì „ ëŒ€í™” ê¸°ë¡: {chat_history}
                """,
                input_variables=["context", "platform", "question", "has_image", "chat_history"]
            )
            
            # ConversationalRetrievalChain ì„¤ì •
            self.qa_chain = ConversationalRetrievalChain.from_llm(
                llm=self.llm,
                retriever=self.vector_store.as_retriever(
                    search_type="mmr",
                    search_kwargs={
                        "k": 3,
                        "fetch_k": 5,
                        "lambda_mult": 0.7
                    }
                ),
                memory=self.memory,
                combine_docs_chain_kwargs={
                    "prompt": PromptTemplate(
                        template="""ë‹¹ì‹ ì€ ì†Œì…œ ë¯¸ë””ì–´ ë§ˆì¼€íŒ… ì „ë¬¸ê°€ìž…ë‹ˆë‹¤.
                        
                        ë‹¤ìŒ ë¬¸ì„œë“¤ì„ ì°¸ê³ í•˜ì—¬ ì£¼ì–´ì§„ ì½˜í…ì¸ ë¥¼ ë³€í™˜í•´ì£¼ì„¸ìš”:
                        
                        {context}
                        
                        ì§ˆë¬¸: {question}
                        
                        ê° í”Œëž«í¼ì˜ íŠ¹ì„±ì„ ê³ ë ¤í•˜ì—¬ ìµœì í™”ëœ ì½˜í…ì¸ ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.
                        ì´ì „ ëŒ€í™” ê¸°ë¡: {chat_history}
                        """,
                        input_variables=["context", "question", "chat_history"]
                    )
                },
                return_source_documents=True,
                verbose=True
            )

    def _initialize_vector_store(self):
        """ë²¡í„° ì €ìž¥ì†Œ ì´ˆê¸°í™” ë° ë¬¸ì„œ ë¡œë“œ"""
        try:
            pdf_path = "./data/pdf"
            if not os.path.exists(pdf_path):
                os.makedirs(pdf_path)
                logger.info(f"Created directory: {pdf_path}")
            
            documents = []
            
            # PDF íŒŒì¼ ë¡œë“œ
            if os.path.exists(pdf_path):
                loader = DirectoryLoader(
                    pdf_path,
                    glob="**/*.pdf",
                    loader_cls=PyPDFLoader
                )
                pdf_documents = loader.load()
                documents.extend(pdf_documents)
                logger.info(f"Loaded {len(pdf_documents)} PDF documents")
            
            # ê¸°ë³¸ ë§ˆì¼€íŒ… ê°€ì´ë“œ ë¬¸ì„œ ì¶”ê°€
            marketing_docs = self._get_default_marketing_docs()
            documents.extend(marketing_docs)
            logger.info("Added default marketing documents")
            
            if not documents:
                logger.warning("No documents loaded")
                return None
            
            # ë¬¸ì„œ ë¶„í• 
            texts = self.text_splitter.split_documents(documents)
            logger.info(f"Split documents into {len(texts)} chunks")
            
            # ë²¡í„° ì €ìž¥ì†Œ ìƒì„±
            store = Chroma.from_documents(
                documents=texts,
                embedding=self.embeddings,
                persist_directory="./data/vector_store"
            )
            store.persist()  # ë²¡í„° ì €ìž¥ì†Œ ì˜êµ¬ ì €ìž¥
            logger.info("Vector store initialized and persisted")
            
            return store
            
        except Exception as e:
            logger.error(f"Vector store initialization failed: {str(e)}")
            return None

    def _get_default_marketing_docs(self) -> List[Document]:
        """ê¸°ë³¸ ë§ˆì¼€íŒ… ê°€ì´ë“œ ë¬¸ì„œ ìƒì„±"""
        return [
            Document(
                page_content="""
                Instagram ë§ˆì¼€íŒ… ê°€ì´ë“œ:
                - ê°ì„±ì ì´ê³  ì‹œê°ì ì¸ í‘œí˜„ ì‚¬ìš©
                - í•´ì‹œíƒœê·¸ëŠ” 5-10ê°œê°€ ì ë‹¹
                - ì´ëª¨ì§€ë¡œ í¬ì¸íŠ¸ ì¶”ê°€
                - ì§§ì€ ë‹¨ë½ìœ¼ë¡œ êµ¬ì„±
                """,
                metadata={"source": "default_guide", "platform": "instagram"}
            ),
            Document(
                page_content="""
                Facebook ë§ˆì¼€íŒ… ê°€ì´ë“œ:
                - ìƒì„¸í•˜ê³  ì¹œê·¼í•œ í†¤ ìœ ì§€
                - ìŠ¤í† ë¦¬í…”ë§ ë°©ì‹ìœ¼ë¡œ ì„œìˆ 
                - ì´ëª¨ì§€ ì ì ˆížˆ í™œìš©
                - ë…ìžì™€ì˜ ìƒí˜¸ìž‘ìš© ìœ ë„
                """,
                metadata={"source": "default_guide", "platform": "facebook"}
            ),
            Document(
                page_content="""
                Thread ë§ˆì¼€íŒ… ê°€ì´ë“œ:
                - ê°„ê²°í•˜ë©´ì„œë„ ëŒ€í™”ë¥¼ ìœ ë„í•˜ëŠ” í†¤
                - ìœ„íŠ¸ìžˆ í‘œí˜„ ì‚¬ìš©
                - í•´ì‹œíƒœê·¸ 2-3ê°œë§Œ ì‚¬ìš©
                - ëŒ€í™”í˜• ë§ˆë¬´ë¦¬
                """,
                metadata={"source": "default_guide", "platform": "thread"}
            )
        ]

    def generate_enhanced_post(self, original_post: str, target_platform: str, has_image: bool) -> str:
        try:
            if not self.vector_store:
                logger.warning("Vector store not initialized, falling back to basic conversion")
                return convert_post(original_post, target_platform, has_image)

            # í”Œëž«í¼ë³„ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì„¤ì •
            platform_prompts = {
                "Instagram": """
                    ë‹¤ìŒ ì½˜í…ì¸ ë¥¼ Instagram í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•´ì£¼ì„¸ìš”:
                    - ê°ì„±ì ì´ê³  ì‹œê°ì ì¸ í‘œí˜„ ì‚¬ìš©
                    - í•´ì‹œíƒœê·¸ 5-10ê°œ í¬í•¨
                    - ì´ëª¨ì§€ ìžì—°ìŠ¤ëŸ½ê²Œ ì‚¬ìš©
                    - ì§§ì€ ë‹¨ë½ìœ¼ë¡œ êµ¬ì„±
                    
                    ì›ë³¸: {original_post}
                    """,
                "Facebook": """
                    ë‹¤ìŒ ì½˜í…ì¸ ë¥¼ Facebook í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•´ì£¼ì„¸ìš”:
                    - ìƒì„¸í•˜ê³  ì¹œê·¼í•œ í†¤ ì‚¬ìš©
                    - ìŠ¤í† ë¦¬í…”ë§ ë°©ì‹ìœ¼ë¡œ ì„œìˆ 
                    - ì´ëª¨ì§€ ì ì ˆížˆ í™œìš©
                    - ë…ìžì™€ì˜ ìƒí˜¸ìž‘ìš© ìœ ë„
                    
                    ì›ë³¸: {original_post}
                    """,
                "Thread": """
                    ë‹¤ìŒ ì½˜í…ì¸ ë¥¼ Thread í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•´ì£¼ì„¸ìš”:
                    - ê°„ê²°í•˜ë©´ì„œë„ ëŒ€í™”ë¥¼ ìœ ë„í•˜ëŠ” í†¤
                    - ìœ„íŠ¸ìžˆëŠ” í‘œí˜„ ì‚¬ìš©
                    - í•´ì‹œíƒœê·¸ 2-3ê°œë§Œ ì‚¬ìš©
                    - ëŒ€í™”í˜• ë§ˆë¬´ë¦¬
                    
                    ì›ë³¸: {original_post}
                    """,
                "Blog": """
                    ë‹¤ìŒ ì½˜í…ì¸ ë¥¼ ë¸”ë¡œê·¸ í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•´ì£¼ì„¸ìš”:
                    - ì •ë³´ì„± ì½˜í…ì¸  ê°•í™”
                    - ëª…í™•í•œ ë‹¨ë½ êµ¬ë¶„
                    - í‚¤ì›Œë“œ ê°•ì¡°
                    - SEOë¥¼ ìœ„í•œ íƒœê·¸ í™œìš©
                    
                    ì›ë³¸: {original_post}
                    """
            }

            # í•´ë‹¹ í”Œëž«í¼ì˜ í”„ë¡¬í”„íŠ¸ ê°€ì ¸ì˜¤ê¸°
            prompt_template = platform_prompts.get(target_platform)
            if not prompt_template:
                logger.warning(f"Unknown platform: {target_platform}")
                return convert_post(original_post, target_platform, has_image)

            # RAG ì²´ì¸ì„ í†µí•œ ì‘ë‹µ ìƒì„±
            response = self.qa_chain({
                "question": prompt_template.format(original_post=original_post)
            })

            if not response or not response.get("answer"):
                logger.warning("No response from RAG chain")
                return convert_post(original_post, target_platform, has_image)

            # ì‘ë‹µì—ì„œ ë¶ˆí•„ìš”í•œ í…ìŠ¤íŠ¸ ì œê±°
            answer = response["answer"].strip()
            
            # ë©”íƒ€ í…ìŠ¤íŠ¸ ì œê±°
            answer = self._clean_response(answer, target_platform)
            
            return answer

        except Exception as e:
            logger.error(f"Post generation failed: {str(e)}")
            return convert_post(original_post, target_platform, has_image)

    def _clean_response(self, response: str, platform: str) -> str:
        """ì‘ë‹µì—ì„œ ë¶ˆí•„ìš”í•œ ë©”íƒ€ í…ìŠ¤íŠ¸ ì œê±°"""
        # ì œê±°í•  í”„ë¦¬í”½ìŠ¤ íŒ¨í„´ë“¤
        prefixes_to_remove = [
            f"{platform} ë§ˆì¼€íŒ… ê°€ì´ë“œì— ë§žì¶˜ ì½˜í…ì¸ :",
            f"{platform} ì½˜í…ì¸  ë³€í™˜",
            f"{platform} í˜•ì‹ìœ¼ë¡œ ë³€í™˜ëœ ì½˜í…ì¸ :",
            "ë³€í™˜ëœ ì½˜í…ì¸ :",
            "###",
            "---"
        ]
        
        # ì œê±°í•  ì„œí”½ìŠ¤ íŒ¨í„´ë“¤
        suffixes_to_remove = [
            "ì´ë ‡ê²Œ ë³€í™˜ëœ ë‚´ìš©ì€",
            "ì´ë ‡ê²Œ ê°ì„±ì ì´ê³ ",
            "---",
            "###"
        ]
        
        result = response
        
        # í”„ë¦¬í”½ìŠ¤ ì œê±°
        for prefix in prefixes_to_remove:
            if result.startswith(prefix):
                result = result[len(prefix):].strip()
        
        # ì„œí”½ìŠ¤ ì œê±°
        for suffix in suffixes_to_remove:
            if result.endswith(suffix):
                result = result[:-len(suffix)].strip()
        
        # ì—¬ëŸ¬ ì¤„ì˜ ê³µë°±ì„ í•˜ë‚˜ì˜ ì¤„ë°”ê¿ˆìœ¼ë¡œ ë³€ê²½
        result = '\n'.join(line.strip() for line in result.splitlines() if line.strip())
        
        return result

    def add_pdf_document(self, pdf_path: str) -> bool:
        """ìƒˆë¡œìš´ PDF ë¬¸ì„œë¥¼ ë²¡í„° ì €ìž¥ì†Œì— ì¶”ê°€"""
        try:
            if not os.path.exists(pdf_path):
                logger.error(f"PDF file not found: {pdf_path}")
                return False
                
            loader = PyPDFLoader(pdf_path)
            pages = loader.load()
            texts = self.text_splitter.split_documents(pages)
            
            if not texts:
                logger.warning(f"No text extracted from PDF: {pdf_path}")
                return False
            
            if self.vector_store:
                self.vector_store.add_documents(texts)
                self.vector_store.persist()  # ë³€ê²½ì‚¬í•­ ì €ìž¥
                logger.info(f"Successfully added PDF document: {pdf_path}")
                return True
                
            logger.error("Vector store not initialized")
            return False
            
        except Exception as e:
            logger.error(f"Failed to add PDF document: {str(e)}")
            return False

    def clear_memory(self):
        """ëŒ€í™” ê¸°ë¡ ì´ˆê¸°í™”"""
        try:
            self.memory.clear()
            logger.info("Conversation memory cleared")
        except Exception as e:
            logger.error(f"Failed to clear memory: {str(e)}")

def main():
    try:
        # Initialize Instagram API
        instagram_api = InstagramAPI()

        # Fetch recent Instagram posts
        media_items = instagram_api.get_user_media(limit=5)
        formatted_posts = instagram_api.format_posts(media_items)

        print(f"Total posts fetched: {len(formatted_posts)}")
        for i, post in enumerate(formatted_posts, 1):
            print(f"\nPost {i}:")
            print(f"ID: {post['id']}")
            print(f"Type: {post['media_type']}")
            print(f"Caption: {post['caption'][:50]}..." if len(post['caption']) > 50 else f"Caption: {post['caption']}")
            print(f"Media URLs: {', '.join(post['media_urls'])}")
            print(f"Permalink: {post['permalink']}")
            print(f"Timestamp: {post['timestamp']}")

        # Instagram í†µê³„ ê°€ì ¸ì˜¤ê¸°
        instagram_stats = instagram_api.get_user_statistics()
        print("\nInstagram Statistics:")
        print(f"Total Posts: {instagram_stats['total_posts']}")
        print(f"Post Types: {instagram_stats['post_types']}")
        print("Popular Hashtags:")
        for tag, count in instagram_stats['popular_hashtags']:
            print(f"  #{tag}: {count}")
        print("Peak Posting Hours:")
        for hour, count in instagram_stats['peak_posting_hours']:
            print(f"  {hour}:00 - {count} posts")

    except Exception as e:
        print(f"An error occurred: {str(e)}")


if __name__ == "__main__":
    main()